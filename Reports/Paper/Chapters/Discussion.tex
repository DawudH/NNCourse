\section{Discussion}

Defining a quantitative performance measure of the networks is outside of the scope of this paper. Therefore a comparison of the results of the five architectures is mainly done by sight.

%comparison of datasets fruit and landscape, on compact net . 
\subsection{Comparing datasets}
The compact network is the only network that was trained on both the landscape dataset and the fruit dataset. In figure \ref{fig:blur} the final results after 20 epochs on the landscape dataset can be seen. The algorithm seems to color landscapes to good standards. However it is concluded that landscape datasets offer less strain on the CNN, because landscapes contain only a small set of features coupled to colors. For example, trees, sea and grass cover a large amount of the landscape dataset, as opposed to the many classes of fruit in the fruit-dataset. In addition, images of landscapes offer a very low amount of spatial transformation. Strawberries and bananas come in many orientations and shapes, whereas landscapes always have a horizon where the top part is blue and the bottom part is green or brown. Therefore the landscape dataset is labeled to be a low-level test-case of the CNNs, and the fruit-dataset is used as a better performance measure for the CNNS.

%discussion on difference between YcbYcr and Cielab Effect of blur
\subsection{Comparing colorspace and effect of blur}

%overfitting of vgg16 so dataset too small
% using compact architectures better results
\subsection{Comparing network complexity}
Another aspect that can be noted is that Networks are prone to overfitting on the small dataset that is used. The fruit dataset consisted of 12000 unique images for training and 2000 images for validation. The most complex network is the VGG16 + classification network, this architecture has 512 feature maps in the bottleneck of the network. As can be seen in \ref{fig:overfit}, the VGG16 + classification network is evidently overfitting after 15 epochs. The network therefore is not a good standard to compare the different features of the architectures, since it is limited by the small size of the dataset. More compact architectures offer a better view of comparing network features for best results. However, more complex networks such as the VGG16 + classification network most likely have highest performance among the networks when the dataset is enlarged, since more features can be extracted. For further research, an interesting topic would therefore be to enlarge the dataset and train more complex networks such as the  VGG16 + classification network. Simple methods to expand the dataset can be to add more transformations on the existing fruit dataset. At the moment only mirroring is used to double the dataset. Cropping and other spatial transformations of the images would be an excellent method to enlarge the dataset and to prevent overfitting of the network. It is interesting to remark that the test dataset is chosen to explicitly contain fruit in order to simplify the problem while still offering a good benchmark for network architectures. Also small datasets come with smaller computational expenses. However it can be concluded now that more complex networks would likely be a good candidate to be trained on full image datasets which offer a rich amount of categories. For example IMAGENET \cite{deng2009imagenet} would be an excellent candidate.
%comparison between dilation, concat and dilation+concat
\subsection{Comparing feature localization}
As can be seen in figure \ref{fig:results}, most of the architectures offer a satisfactory performance of the fruit dataset. However interesting differences can be seen in this comparison of CNN architectues. For example the effect of class-rebalancing is very evident in the comparison between the regression and classification networks. Class-rebalancing was implemented on all the classification networks with a $\lambda$ of 0.3. The images in the classification networks offer more vibrant colors, where the colors in the regression network are more dull. The fruit dataset is biased towards green red and especially grey colors. Class-rebancing therefore offers a good solution to add in more exotic colors in the images which are less comon in the dataset.

Another interesting differenc to notice, is the difference between the dilated and concatenated networks. It can be seen that the networks sometimes still have difficulty with the correct localization of the color. Both the dilated and the concatenated networks show a substantial amount of color artifacts in the output image. For example, in the middle image consisting of the fruit bowl, both networks color a part of the kiwi red. This originates from the red used in the neigbouring strawberry, the network is not accurate enough in localization of the strawberry. However a difference can be noted between the two configurations: the dilated version shows less artifacts then the concatenated networks. An explanation could be that the network gets steered to the correct colors already by mapping the greyscale value to a reduced set of colors. For example, the image of strawberries in the second picture in figure \ref{fig:moreresults} in the appendix, shows a strawberry colored green. However the network has no idea whether this stawberry needs to be colored green or red, still it picks the color green. All the five networks show this behaviour, this is some evidence that the network uses some of the grayscale input to map a color to the image. The concatenated network use a lot more of this information since the input layers are concatenated with the output pipeline. Thus instead of picking a 'realistic' color, the concatenated network maps more of the grayscale color information in the outline pipeline of the network, hence often creating objects that are partially colored ith the ground truth color, as can be seen in the fifth row of figure \ref{moreresults}.
%final comparison between all architectures, show massive image.
\subsection{Final comparison}

%conclusion on best network







%Gaussian blur
%It is clear that the case with $\sigma=0$, i.e. no blur, the network was unable to find any colors. Using a blur radius of $\sigma=3$ or $\sigma=5$ leads to much better results. Between these two, the $\sigma=3$ case was chosen to be the best, since it seems more inclined to pick more saturated colors. A too high blur radius will also make the training less effective since the link between texture and color becomes weaker at the edges of objects.