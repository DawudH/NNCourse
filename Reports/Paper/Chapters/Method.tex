\section{Method} \label{sec:method}
%Method
%Prerequisites (things used by all networks)
%Data set (fruit), (landscape)
%Input size (128X128)
%
%Color spaces + which one to use :
%RGB (luminosity not separated from color)
%HSV (circular domain)
%YCbCr (OK)
%CIELab (OK)
%first layer as input, second and third layer as output
%
%Architecture (not what it is but why WE use it)
%General discription (how to colorize an image with an NN)
%Features used by all networks
%ReLu, weight initialization, padding, kernel size
%Feature extraction
%Reconstruct
%Concatenate
%Dilated convolution
%Color generation
%Two feature maps
%blur
%Classification
%k-means
%annealed mean
%gaussian blur
%
%Loss function
%Squared error
%Class rebalancing (histogram dataset)
%Cross entropy
%Class rebalancing
%Architectures used:
%Dahl, Compact, Dahl_classifier, Dahl_zhang, Zhang
%
%Training method
%nesterov momentum
%adadelta

This section covers the different methods used to get an answer to the question of which CNN setup is best in colorization of images. Firstly, the chosen dataset will be explained. In addition the input size and the different colorspaces used will be explained. Next the different architectures used will be explained. Features extraction, training parameters, loss functions, and general architectures of the chosen networks will be evaluated. Finally the test setup will be explained.

\subsection{Dataset and Input}

Large training and validation datasets are required for training and validation of the CNNs.
Some restrictions on the datasets had to be made due to limitations in computational power, resulting in a selection of images based on a certain category; fruit and landscape images.

The landscape dataset is used as a low-level test case, images of landscapes offer less distinct features coupled to a mapping of the colors, e.g. most landscapes are green at the bottom and blue at the top. The high-level test case consists of the fruit dataset.
Fruit is a category which has distinct colors corresponding to distinct features. Therefore, in order to color fruit correctly, the network has to learn to link colors to distinct features in the input, straining the networks requirements. Also many fruit classes are linked to ambiguous colors, i.e. green or red apple. Taking fruit as the dataset is therefore an excellent method to check whether the network is able to counter the 'averaging' problem also mentioned before.

The datasets are generated using the popular image website Flickr\footnote{\url{https://www.flickr.com/}}.
Using their freely available API, a program was made that retrieved images in the required resolution and kept track of images retrieved, to avoid duplicates. The datasets retrieved had to be checked on incorrect images. To solve this problem a web application was made that enabled us to check images on defects. The checked datasets are collected in batches and stored in Python Numpy arrays as input for the network.
This resulted in 2 datasets, which are summarized in table \ref{tab:dataset}.

\begin{table}[h!]
	\centering
	\caption{Datasets used for training and validation of the various networks}
	\label{tab:dataset}
	\begin{tabular}{|l|l|l|}
		\hline
		\textbf{Dataset}   & \textbf{\# Training Images} & \textbf{\# Validation images} \\ \hline \hline
		Fruit     & 6000            & 1000              \\ \hline
		Landscape & 34000           & 5000              \\ \hline
	\end{tabular}
\end{table}

The network input are 128x128 pixel grayscale images, which are propagated through the network in batches. Using batches reduces computation time since better use is made of the parallelization of modern computer architectures. Most importantly the gradients calculated using batches are a better estimate of the gradient for the entire training set, thus leading to a more stable gradient descent \cite{ioffe2015batch}. Making the batch size too large makes the search to the global minimum less stochastic, thus making it converge to a local optimum more quickly, and in general converging less quickly. 

The network is trained to be able to colorize an image, but the way it outputs the colorized image is dependent on the color space used during training. There are several options available:\\

\textbf{RGB} This widely used color space specifies an intensity for the channels red, blue and green. The biggest drawback of using RGB is that the color is not separated from the luminosity. In this way, the network needs not only to output a hue and saturation of a color, but also the luminosity itself. This makes it a much tougher challenge to output an image that resembles a colorized version of the grayscale image. A visualization of the RGB space is given in figure \ref{fig:RGB}.

\textbf{HSV} Specifying the hue, saturation and value, uncouples the luminosity (value) from the color (hue and saturation). Furthermore decoupling the saturation could allow specifically tackling the sepia and saturation problem as described in section \ref{sec:intro}. However, the color space is periodic in the hue axis. From the numerical perspective, this leads to an ambiguous error specification, since for one difference between target and network output color, two directions of improvement are equally valid, thus a very complicated loss function is required, where the network needs to learn this circular property of the hue axis. This colorspace is used for training on the compact network architecture, however the network did not perform as good as when it was trained on either YCbCr or CIELab. Therefore the HSV colorspace was omitted.

\textbf{YCbCr} The Y channel contains the luminosity of the image, while Cb and Cr layers are the chroma blue and chroma red layers respectively. While providing a separate luminosity channel, it was found that for different Y values, a given Cb and Cr combination does not specify the same color. This makes the error in the color specification dependent on the luminosity of the image. A visualization of the color space can be found in \ref{fig:YCbCr}.

\textbf{CIELab} Similarly to the YCbCr color space, the L channel specifies the luminosity, the a and b layers the color. In contrast with the YCbCr space, a given a and b value specify a color, while the luminosity only determines how light the color is. However, for a single L layer not all a and b value translate to colors that can be represented in the RGB color space (as displayed by computer monitors). The CIELab color space is visualized in figure \ref{fig:CIELab}.\\

To summarize, the HSV, YCbCr and CIELab color spaces all have the possibility of using one channel as input to the neural network, while requiring only two outputs of the network that instead of three with the RGB color space. Combined with the input, these two outputs allow reconstructing a color image. According to Iizuka, the CIELab color space has been found to give the most correct results for a colorization network\cite{IizukaSIGGRAPH2016}. Considering the disadvantages of the HSV color space, the YCbCr and CIELab color spaces are compared during the present research.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth,trim={125px 75px 125px 75px},clip]{RGB}
		\caption{The RGB colorspace}
		\label{fig:RGB}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth,trim={125px 75px 125px 75px},clip]{YCbCr}
		\caption{The RGB colorspace represented in the YCbCr colorspace}
		\label{fig:YCbCr}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.32\textwidth}
		\includegraphics[width=\textwidth,trim={175px 75px 150px 75px},clip]{CIELab}
		\caption{The RGB colorspace represented in the CIELab colorspace}
		\label{fig:CIELab}
	\end{subfigure}
	\caption{The different colorspaces used in the different networks}\label{fig:animals}
\end{figure}

\subsection{Convolutional neural network properties}
\label{sec:nnproperties}

Convolutional neural networks contain a vast amount of properties that determine the behaviour of the network. Structure of the loss function and the model architecture being the most important properties. To compare different architectures, it is important to keep the hyperparameters the same as much as possible, resulting in a fair comparison between CNNs. The choices made in the experiment setup which are affiliated with the CNN properties will be explained here.
\begin{wrapfigure}{R}{0.5\textwidth}
	\vspace{-20pt}
	\begin{center}
		\includegraphics[width=0.49\textwidth,trim={75px 0px 75px 0px},clip]{K_nearest}
	\end{center}
	\caption{K nearest neighbors}
	\label{fig:knearest}
\end{wrapfigure}


A few parameters are held constant over all the test cases. The parts of the network (or the complete network) that are initialized without pre-trained weights are randomly initialized. This is done using a Glorot Uniform distribution \cite{Glorot}. This weight initialization method samples from a uniform distribution with its variance scaled depending on the ingoing and outgoing data. 

The intermediate layers of all the network consist of convolutional layers. All convolutional layers use ReLu non-linearities \cite{nair2010rectified}, except for the final output layers, they are different per architecture. 
Kernel sizes in the convolutional layers of the network are of size 3x3. This kernel size is based upon VGG16's \cite{Simonyan} kernel size, as is used in the other reference networks \cite{Dahl}\cite{Zhang} \cite{ioffe2015batch}.
Stride in the convolutional layers is 1x1 by default. However, in the CNNs using dilation in the output pipeline, stride is used as a way to downsample the resolution of the image, comparable to the function of max pooling, but retaining more spatial properties of the input image \cite{yu2015multi}. Padding is required when using convolutional layers due to the fact that border information of the image is lost when convolving. This property is set to keep the output resolution the same as the input resolution. Zero valued padding is used when conventional convolutions operations are performed. Symmetric valued padding is used when dilated convolutions are used, again the amount of padding is chosen to keep the output dimensions the same.\\


\subsubsection{Feature Extraction}
To recognize certain objects in grayscale images, object dependent features are extracted. This is done in the first part of the convolutional neural network, also known as the input pipeline. To extract features convolutional layers are used. Each convolutional layer convolves the input of the layer with a kernel to a set of feature maps. Max-pooling is often used to make feature extraction spatially invariant to input transformations \cite{Simonyan}. The kernels in the incoming pipeline of the CNN can be chosen to be initialed from scratch, or to be initialed with pre-trained weights. For example VGG16 \cite{Simonyan} has shown excellent results in image classification, and therefore already has a large amount of feature extraction available. In this paper a comparison is made between using architectures which are completely trained from scratch, and architectures which use the input pipeline of the VGG16 network to initialize the convolutional kernels.\\

\subsubsection{reconstruction}
In the input pipeline of the network, the image is reduced to a set of features, containing decreased amounts of spatial information. To be able to colorize the image, localization of these features is required. For reconstruction, various methods are used to retrieve the original image resolution.

First of all, the feature maps can be upscaled using linear upscaling. However, to retrieve the original image not only the features but also the localization of the features needs to be done. There are multiple ways of retrieving the spatial information of the image \cite{Charpiat} \cite{Zhang}. The methods tested in these paper consist of residual auto-encoders and dilated convolutions \cite{yu2015multi}.

The use of a residual auto-encoder has been demonstrated by \cite{Dahl}. After the bottleneck of the CNN each layer is upscaled and concatenated with the parallel layer in the input pipeline of the network. This way global features are merged with the localization of those features. This process is repeated after each upscale layer until the output of the network is reached, as can be seen in figure \ref{fig:dahlnetwork}.

%This is done by concatenating the layers of the same resolution before the bottleneck with the upscaled features after the bottleneck. Then an convolutional layer is used to merge these features in feature maps of the upscaled resolution. This process is repeated until the original image resolution is retrieved.
%
Another technique used is the use of dilated convolutions \cite{yu2015multi}. Zhang et al. use strides in the incoming pipeline of the CNN, which causes less loss of spatial information. The bottleneck of the network consists of 28x28 feature maps. The output pipeline of the network is initialized by two layers of dilated convolutions, where each layer consists of 3 consecutive dilated convolutions. Using dilated convolutions has the effect that the input pipeline can compress the image to a larger size leaving more spatial information in the bottleneck of the image. The receptive field of dilate


\subsubsection{Color Generation}
In the final layers of the network, the original image color layers have to be reconstructed. Two different methods are used throughout the paper: construction using two feature maps and classification.

The construction using two feature maps is a direct result of retrieving the original image resolution through the reconstruction of the image. As a final layer, a convolutional layer is used that maps to two feature maps, which represent the two color layers that are finally used to create a colorized image.

As an aid to the colorization process, a Gaussian Blur is used on the color layers of the original image. Colorization does not require precise pixel by pixel colorization, because colors in images mostly appear in sets of pixels. When blurring, this enables the network to more easily converge towards a solution by reducing the noise in the color of the pixel set. Note that blurring the color layers does not reduce the image fidelity. This is due to the fact that the luminosity layer contains the details and contrast of the image, which is subsequently merged with the output color layers of the network. This makes the colorization problem into a regression problem, where a continuous function is sought which maps a grayscale input to the two values that determine color in the output.

The other method used is a classification approach to colorization: for a given input image, every pixel in the output image falls within one of a few predefined color classes. For the classification networks, the colorspace CIElab is used. The ab colorspace is discretized into $n$ classes, each corresponding to a discretized bin in the original ab colorspace. Classification applies a loss function per pixel in the image, so each pixel has a corresponding target vector. Instead of generating one-hot vectors per pixel, probability distributions are generated, assigning probabilities to multiple colorbins. This way the network can make use of the similarity of different colors in the ab colorspace. Using a target vector which has a distribution over the classes instead of a one-hot vector, helps to steer the network to the correct colors. In addition the use of a color distribution more closely resembles the nature of the colorization problem, since not every object is linked to a single color, but rather a distribution of possible colors. 

\begin{wrapfigure}{R}{0.5\textwidth}
	\vspace{-20pt}
	\begin{center}
		\includegraphics[width=0.40\textwidth,trim={75px 0px 75px 0px},clip]{K_nearest}
	\end{center}
	\caption{An illustration of the K-nearest neighbour algorithm. The red dots represent the centers of the colorbins in the discretized colorspace. The blue dot represents the ground truth ab value of the target pixel. The yellow dots correspond the the K-nearest neighbour colorbins. The class probabilities of the targetvector are generated by applying a gaussian blur on the distances to the K-nearest neigbour colorbins.}
	\vspace{-10pt}
	\label{fig:k_nearest}
\end{wrapfigure}

Each target vector is generated by applying a gaussian blur over the k-nearest neighbour colorbins of the exact ab value, as can be seen in \ref{fig:k_nearest}. The result of these operations over one image is a set of $128*128=16384$ target vectors, each consisting of a probability distribution over the $n$ colorbins. 
The final colorized version of the image is created by processing the probability distribution per pixel. Implementing an annealed mean to pick the best color from the probability distribution allows setting the degree with which to pick either the mode or the mean of the distribution \cite{Zhang}. The annealed mean operation can be seen in \ref{eq:anmean}, where \textbf{z} is the output probability vector of one pixel, T is the annealed mean temperature, and q is the total number of colorbins.

\begin{equation}
f_{T}(\textbf{z})=\frac{exp(\textbf{z}/T)}{\sum_q^{}exp(\textbf{z}_{q}/T)}
\label{eq:anmean}
\end{equation}

Both the two feature maps and classification approach are tried during the present research.



\subsubsection{Loss Function}
During the present research, several loss functions were used, depending on the color generation method used.

The two feature maps represent the color space layers of the to be reconstructed image, depending on the selected colorspace. The loss function is then defined as the sum of the squared distances between the output of the network and the original image color layers. This can be seen in equation \ref{eq:squaredsum}, where $p$ is the prediction of the network and $t$ is the target. The distances are calculated per pixel and per color channel.

\begin{equation}
\label{eq:squaredsum}
L = \sum_{pixels} \sum_{channels}(p - t)^2
\end{equation}

To enable the network to use more saturated colors, color rebalancing may be applied to the loss function. This color rebalancing penalizes the the network for selecting desaturated colors, by exponentially increasing the loss function when converging towards desaturated solutions. However, it was found that this does not produce desired results, probably due to the fact that it makes no use of the actual probability of the colors, thus leading to a crude approximation of the histogram. For the two feature maps output, the standard sum squared error as in equation \ref{eq:squaredsum} is used.

For classification, categorical cross-entropy is used. This is done between the predicted color classification of the network per pixel and the target classification per pixel of color layers of the image. The categorical cross-entropy function is defined in equation \ref{eq:crossentropy}, where $t_{i,j}$ is the target classification, where $i$ specifies the pixel, and $j$ its respective probability distribution of the classified color. 
$p_{i,j}$ is the predicted classification, where $i$ depicts the predicted pixel. $j$ its estimated probability distribution of the classified color, by the network.

\begin{equation}
\label{eq:crossentropy}
L_{i} = -\sum t_{i,j}\log(p_{i,j})
\end{equation}

Due to the fact that in the training set the lower a and b values are highly represented, after training a bias will result towards picking lower a and b values during colorization, thus leading to undersaturated results. 
Class rebalancing is added to assist the network in coping with this existing skewness of the training data set. This is done by generating a histogram of the discrete colors in the dataset, using this information to balance the loss function with respect to each color. 
This histogram is then fused with a uniform distribution to deal with the fact that some of the colors in the histogram are so underrepresented that they have too big of an influence on the color rebalancing. To keep the color rebalancing within reasonable bounds, its expected value is kept equal to one. 
The class rebalancing factor is given by equation \ref{eq:classrebalance}. The complete loss function of the cross-entropy is given by equation \ref{eq:lossZhang}, while the mean is also taken over the batch.

\begin{equation}\label{eq:classrebalance}
V = ((1 - \lambda)\cdot H + \lambda)^{-1}
\end{equation}
 
\subsection{Training method}
For convolutional neural networks training, stochastic gradient decent (SGD), sometimes together with momentum, is commonly used for updating the weights and biases \cite{IizukaSIGGRAPH2016}\cite{Simonyan}. The used hyper-parameters, especially the learning rate, requires careful tuning when using SGD. Often a scheduled learning rate, that is monotonically decreasing depending on the epoch, results in the best results. Since the focus of the present research lies not on what training method parameters are required, ADADELTA was used to automatically set the learning rate for every parameter\cite{zeiler2012adadelta}. Furthermore, Nesterov momentum was applied to lower the probability of getting stuck in a local minimum\cite{sutskever2013importance}.


\subsection{Final choice of model architectures}





\begin{wrapfigure}{R}{0.5\textwidth}
	\vspace{-20pt}
	\begin{center}
		\includegraphics[width=0.48\textwidth]{hist}
	\end{center}
	\caption{\color{red} The histogram of the total fruit dataset}
	\vspace{-10pt}
\end{wrapfigure}





